21.05.2023    12:16
Теги: #Python #Neuro 
---
# ***Внутренние ссылки:***

---
# ***Внешние ссылки:***
[Инфо сайт](https://proproprogs.ru/)
[Doc](https://www.tensorflow.org/?hl=ru)
[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)
---
# ***Заметка:***

# Примеры применения. Установка
![[Pasted image 20230521122642.png]]
Эффективность здесь достигается за счет устранения дублирования при вычислениях (если они возникают), а также за счет распараллеливания независимых операций. 

Кроме того, как мы сейчас увидим, такой граф также позволяет эффективно вычислять и производные функции, что весьма полезно при разработке алгоритмов машинного обучения и, вообще, при решении различных оптимизационных задач.

Общая идея вычислительного графа – представить целевую функцию набором элементарных математических операций (сложение, вычитание, умножение, деление) и стандартного набора функций (sin, cos, exp, log, sqrt и т.д.). Существует теорема, доказывающая, что любую непрерывную функцию можно представить с заданной точностью набором таких математических действий, то есть, с помощью вычислительного графа.

Но какое все это имеет отношение к нейронным сетям? Самое прямое. Любую нейронную сеть можно представить в виде вычислительного графа. Предположим, мы создаем простую полносвязную трехслойную НС:
![[Pasted image 20230521122926.png]]![[Pasted image 20230521122939.png]]

## Вычисление производных с помощью графа
![[Pasted image 20230521123320.png]]
![[Pasted image 20230521123238.png]]
![[Pasted image 20230521123256.png]]
## Примеры использования Tensorflow

Чтобы увидеть, как вся эта математика реализуется в Tensorflow, выполним вычисления производных этой же функции в той же точке. Для этого я приведу следующую простую программу:

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
 
import tensorflow as tf
 
x = tf.Variable([[2.0]])
y = tf.Variable([[-4.0]])
 
with tf.GradientTape() as tape:
  f = (x + y) ** 2 + 2 * x * y
 
df = tape.gradient(f, [x, y])
 
print(df[0], df[1], sep="\n")
```

Создаем две переменные x и y со значениями 2 и -4. С помощью функции GradientTape() сохраняем все промежуточные значения графа для последующего расчета производных. 
Данные сохраняются в объекте tape (лента). После этого, мы вызываем метод gradient() объекта tape и указываем функцию, а также аргументы, по которым будет производиться вычисление производных. В конце выводим вычисленные значения в консоль:

```python
tf.Tensor([[-12.]], shape=(1, 1), dtype=float32)
tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
```

### Реализация градиентного спуска

Чтобы вы лучше понимали широту использования Tensorflow, приведу еще один пример по реализации стохастического градиентного спуска для вычисления точки минимум функции ![[Pasted image 20230521123508.png]]. Программа выглядит следующим образом:

```python
x = tf.Variable(-1.0)
y = lambda: x ** 2 - x
 
N = 100
opt = tf.optimizers.SGD(learning_rate=0.1)
for n in range(N):
  opt.minimize(y, [x])
 
print(x.numpy())
```

После N=100 итерации SGD получаем значение 0,49999 – очень близкое к оптимальному значению 0,5.

## Установка Tensorflow

```python
pip install tensorflow
```

По умолчанию этот пакет пытается использовать GPU для распараллеливания вычислений. Чтобы Tensorflow мог подключаться к графическому процессору, необходимо на компьютер установить дополнительное программное обеспечение:

## CUDA Toolkit

доступное по адресу:

[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)

Но делать это не обязательно. Если GPU не используется, то Tensorflow все вычисления проводит на CPU – центральном процессоре компьютера. И чтобы он не выдавал при этом лишних предупреждений, в начале программ я записываю следующие две строчки:

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
```

Для проверки корректности установки можно прописать еще две таких строчки:

```python
import tensorflow as tf
print(tf.__version__)
```

#  Тензоры tf.constant и tf.Variable. Индексирование и изменение формы

В Tensorflow это слово можно воспринимать как поток тензоров, и действительно, здесь это, фактически, означает поток вычислений с многомерными матрицами. Да, тензоры в Tensorflow представляются именно как многомерные матрицы, а не математические тензоры. Поэтому те, кто знаком с математикой тензоров следует быть аккуратными с их восприятием при работе в этом пакете. Подробную документацию по ним можно посмотреть по ссылке:

[https://www.tensorflow.org/guide/tensor](https://www.tensorflow.org/guide/tensor)


***Tensorflow 2.x по умолчанию включен режим активного выполнения (executingeagerly). В этом можно убедиться, выполнив команду:***

```python
print(tf.executing_eagerly() )
```

***В консоли увидим значение True. Этот режим очень удобен для небольших проектов, так как позволяет вычислять значения переменных сразу после выполнения соответствующих команд. Это упрощает отладку и обеспечивает наглядность работы с Tensorflow.***

## Определение тензоров

### Переменные tf.constant

Но вернемся к тензорам. В самом простом случае их можно определить как константные, неизменяемые объекты, используя функцию

```python
tf.constant(value, dtype=None, shape=None, name="Const")
```

-   value – значения тензора;
-   dtype – тип данных тензора;
-   shape – размерность тензора;
-   name – имя тензора.

Например, через эту функцию можно задать тензор, содержащий одно число:

```python
a = tf.constant(1)
print(a)
```

В консоли увидим, что это объект типа Tensor со значением 1, не имеющий осей и тип данных int32 – целочисленный:

```python
tf.Tensor(1, shape=(), dtype=int32)
```

Отсутствие осей говорит, что ранг тензора равен нулю. Однако, если явно указать параметр shape с размерностями 1x1:

```python
a = tf.constant(1, shape=(1,1))

то получим уже матрицу с этим размером 1x1:

tf.Tensor([[1]], shape=(1, 1), dtype=int32)

Если же создать тензор на основе одномерного списка:

b = tf.constant([1, 2, 3, 4])
print(b)
```
То у него автоматически появится одна ось, размером в 4 элемента:

```python
tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)
```

Соответственно, для двумерного списка (или матрицы):

```python
с = tf.constant([[1, 2],
                 [3, 4],
                 [5, 6]], dtype=tf.float16)
print(с)
```

получим тензор ранга два (две оси):  
```python
tf.Tensor(  
[[1. 2.]  
 [3. 4.]  
[5. 6.]], shape=(3, 2), dtype=float16)
```
И так далее, мы можем создавать тензоры произвольной размерности. Причем, обратите внимание, в последнем случае мы явно указали тип данных float16 (вещественный 16 бит) и при создании тензора этот тип был применен для всех его элементов. Если же тип не указывается, то он определяется автоматически, исходя из переданных данных. Отсюда следует, что все элементы в тензоре имеют единый тип данных, то есть, здесь нельзя одновременно хранить, например, числа, строки, булевы значения. Только что то одно – один тип. Также следует иметь в виду, что тензоры формируются на основе прямоугольных списков. То есть, число элементов в каждой строке матрицы должно быть одинаковым и это правило должно соблюдаться и в многомерном случае. Хотя, в Tensorflow есть возможность определять неравномерные тензоры (RaggedTensor).

При необходимости мы можем поменять тип данных тензора. Для этого используется функция cast, которая возвращает новый тензор, содержащий те же значения с указанным типом данных:
```python
a2 = tf.cast(a, dtype=tf.float32)
```

Вообще, тензоры построены по аналогии с многомерными матрицами пакета NumPy и многие операции над ними аналогичны операциям в NumPy.

Мало того, любой тензор можно преобразовать в массив NumPy, либо с помощью функции array, либо методом numpy():
```python
import numpy as np
 
np_b1 = np.array(b)
np_b2 = b.numpy()
 
print(np_b1, np_b2, sep="\n")
```
### Переменные tf.Variable

Если же требуется объявить тензор с изменяемыми значениями, то для этого используется класс tf.Variable. Он имеет похожий набор параметров, что и функция tf.constant() и применяется подобным образом:

```python
b = tf.constant([1, 2, 3, 4])
 
v1 = tf.Variable(-1.2)        # переменная из одного числа
v2 = tf.Variable([4, 5, 6, 7], dtype=tf.float32) # переменная через список
v3 = tf.Variable(b)           # переменная через тензор
 
print(v1, v2, v3, sep="\n\n")
```

Чтобы изменить значение переменной, вызывается метод assign() с указанием других величин, например, так:
```python
v1.assign(0)
v2.assign([0, 1, 6, 7])
```

Обратите внимание, что размерность обновляемых данных должна совпадать с размерностью переменной, иначе произойдет исключение (ошибка). При этом, новые данные, как правило, записываются в ту же область памяти, где хранились прежние значения.

По аналогии можно использовать методы:
```python
v3.assign_add([1, 1, 1, 1])   # добавление значений
v1.assign_sub(5)              # вычитание значений
```


```python
v3 = tf.Variable([-1, -2, -3, -4])
```

Мы также получим изменение данных, хранящихся в новом объекте, а старый будет уничтожен сборщиком мусора.

![[Pasted image 20230521123932.png]]

Но именно из-за этого возникает одна существенная проблема. Когда какой-либо тензор передается функции, например:

```python
opt = tf.optimizers.SGD(learning_rate=0.1)
opt.minimize(y, [x])
```

то измененное таким образом значение не будет доступно за пределами этой функции (в основной программе), так как ссылки y и x будет вести к исходным объектам, а не новым. Именно поэтому, при оптимизации каких-либо изменяемых параметров, например, весов в НС, их следует определять как переменные через объект tf.Variable.

Также следует помнить, что создание новой переменной, на основе существующей:

```python
v4 = tf.Variable(v1)
```

создает ее копию, то есть, ссылки v4 и v1 будут вести на разные объекты. Эту конструкцию можно использовать для клонирования переменных тензоров.

Опять же, если нам нужно преобразовать переменную в массив NumPy, то достаточно вызвать метод numpy():

```python
print(v2.numpy())
```

Чтобы посмотреть размерность тензора можно воспользоваться его свойством shape:

```python
print(v2.shape)
```

Возвращает кортеж с размерностями по каждой из осей:

```python
(4,)
```

## Индексирование и срезы

Следующий важный момент – это индексирование элементов тензоров и извлечение срезов. Эти операции реализованы аналогично массивам пакета NumPy. Поэтому за подробной информацией обращайтесь к учебному курсу на этом канале. Я не буду здесь повторяться и лишь сделаю краткий обзор по индексированию и срезам в Tensorflow.

Следует иметь в виду, что по индексам можно только читать данные из тензоров, например, так:

```python
val_0 = v3[0]     # первый элемент
val_12 = v3[1:3]  # элементы со 2-го по 3-й [-2, -3]
```

Причем, переменные val_0 и val_12 будут ссылаться на те же данные, что и тензор v3, то есть, копирования информации здесь не происходит. Это очень удобно с точки зрения производительности и использования памяти, но при этом, нужно понимать, что изменение данных в val_0 и val_12 приведет к изменению и тензора v3:

```python
val_0.assign(10)
print(v3)
```

Увидим значения:

```python
<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([10, -2, -3, -4])>
```

то есть, через val_0 был изменен первый элемент тензора v3.

Далее, Tensorflowподдерживает списочное индексирование с помощью функции gather(), следующим образом:
```python
x = tf.constant(range(10)) + 5
x_indx = tf.gather(x, [0, 4])
print(x, x_indx, sep="\n")
```

На выходе тензор x_indx будет состоять из двух элементов с индексами 0 и 4, взятыми из тензора x.

Если же мы попытаемся обратиться к вектору через список:
```python
val_indx = v2[(1, 2)]
```

то это будет восприниматься как получение элемента из второй строки (индекс 1) и третьего столбца (индекс 2). Поэтому для одномерного тензора данная запись приведет к ошибке. Но если задать двумерный тензор:
```python
v3 = tf.constant([[1, 2, 7], [3, 4, 8], [5, 6, 9]])
val_indx = v3[(1, 2)]
print(val_indx)
```

то в консоли увидим значение 8 (элемент из 2-й строки и 3-го столбца).

Того же самого результата можно добиться и так:
```python
val_indx = v3[1][2]
```

но, обычно, используют первый вариант через кортеж, т.к. он удобнее и нагляднее:
```python
val_indx = v3[0, 1]
```

Если же мы в качестве одного из параметров укажем только один индекс:
```python
val_indx = v3[0]
print(val_indx)
```

то будет возвращена соответствующая строка (в данном случае первая). При этом, размерность тензора val_indx будет shape=(3,):
```python
tf.Tensor([1 2 7], shape=(3,), dtype=int32)
```

то есть, он представляется уже одномерным массивом.

Для считывания определенного столбца целиком, следует использовать механизм срезов, например, так:
```python
val_indx = v3[:, 1]
```

получим второй столбец в виде вектора [2, 4, 6].

Напомню, что срезы определяются синтаксисом:
```
start:stop:step
```

поэтому для извлечения, например, только первых двух элементов из последнего 3-го столбца можно записать:
```python
val_indx = v3[:2, 2]
```
или так:
```python
val_indx = v3[:2, -1]
```

Здесь отрицательные индексы определяют нумерацию с конца соответствующей оси.

В целом, вот так выполняется индексирование в Tensorflow и по аналогии применяется к матрицам любых размерностей. Опять же, подробную информацию об индексировании смотрите в курсе по NumPy.

## Изменение формы тензоров

У любого тензора можно поменять форму (число элементов по осям). Например, пусть задан одномерный тензор длиной в 30 элементов:
```python
a = tf.constant(range(30))
```

Его можно привести к другой форме (создать новое представление) в виде матрицы 5x6 элементов. Это делается с помощью функции tf.reshape(), следующим образом:
```python
b = tf.reshape(a, [5, 6])
print(b.numpy())
```

Теперь переменная b ссылается на те же самые данные, что и переменная a, но форма тензора стала другой – матрица 5x6. Так как здесь происходит лишь изменение формы тензора, то это достаточно быстрая операция и ее можно безопасно выполнять и для больших данных.

Обратите внимание, что мы выбрали форму матрицы именно 5x6, которая содержит также 30 элементов, что и начальный вектор. Если указать несогласованные размеры, например:
```python
b = tf.reshape(a, [5, 5])
```
то это приведет к ошибке, так как 5x5 = 25, что меньше, чем 30 – число элементов в векторе a.

Наконец, мы можем по одной из осей не указывать размер (записывая -1), тогда он будет вычислен автоматически, исходя из общего числа элементов:
```python
b = tf.reshape(a, [6, -1])
```

Получим матрицу 6x5.

Функция reshape() позволяет менять форму тензора, но не порядок следования элементов. Это значит, что она не подходит, например, для операции транспонирования, когда строки и столбцы меняются между собой. Для этого следует использовать специальную функцию:
```python
b_T = tf.transpose(b, perm=[1, 0])
```

Здесь параметр perm указывает по каким осям проводить операцию транспонирования.

# Математические операции и функции над тензорами

## Функции автозаполнения

Ниже в таблице представлены основные функции для создания тензоров с указанной размерностью и автозаполнения заданными значениями:

![[Pasted image 20230521124612.png]]
Например, с помощью функции zeros можно создать тензор с нулевыми значениями, следующим образом:

```python
a = tf.zeros((3,3))
print(a)
```

Функция ones() работает аналогичным образом, только вместо нулей записывает единицы:

```python
b = tf.ones((5,3))
print(b)
```

Похожие функции zeros_like() и ones_like() создают тензоры с нулевыми и единичными значениями, но размерности берут из переданного им другого тензора. Например:

```python
c = tf.ones_like(a)
```

создаст матрицу из единиц размером 3x3, так как тензор aимеет такие размеры.

Следующая функция eye() создает матрицу с единицами по главной диагонали и нулевыми значениями вне ее:

```python
d = tf.eye(3)
print(d)
```

После выполнения в консоли увидим:

```python
tf.Tensor(
[[1. 0. 0.]
 [0. 1. 0.]
[0. 0. 1.]], shape=(3, 3), dtype=float32)
```
Если же указать два аргумента:
```python
d = tf.eye(3, 2)
```

то получим аналогичную матрицу, но с размерами 3x2.

Для создания копий тензоров с сохранением их значений можно воспользоваться специальной функцией identity(), например, так:

```python
e = tf.identity(c)
```

Будет создана копия тензора c.

Следующий метод fill() позволяет создавать тензоры с заданным числовым значением:

```python
f = tf.fill((2, 3), -1)
```

Получим тензор 2x3 со значениями -1.

Наконец, функция range() позволяет в Tensorflow генерировать последовательности чисел в заданных границах и с заданным шагом, например, так:

```python
g = tf.range(1, 11, 0.2)
```

## Генерация случайных чисел

В Tensorflow имеется модуль random, отвечающий за генерацию тензоров со случайными значениями.

Н![[Pasted image 20230521124734.png]]

Все эти функции выполняются вполне очевидным способом:

```python
a = tf.random.normal((2, 4), 0, 0.1)  # тензор 2x4 с нормальными СВ
b = tf.random.uniform((2, 2), -1, 1)  # тензор 2x2 с равномерными СВ в диапазоне [-1; 1]
c = tf.random.shuffle(range(10))  # перемешивание последовательности чисел
tf.random.set_seed(1)  # установка зерна датчика случайных чисел
d = tf.random.truncated_normal((1, 5), -1, 0.1)  # тензор 1x5 с ограниченными нормальными СВ
```

## Математические операции в Tensorflow
Предположим, имеются два одномерных тензора (векторы):
```python
a = tf.constant([1, 2, 3])
b = tf.constant([9, 8, 7])
print(a, b, sep="\n", end="\n\n")
```

С ними можно выполнять следующие распространенные операции:

```python
z1 = tf.add(a, b)       # сложение
z2 = a + b              # сложение
z1 = tf.subtract(a, b)  # вычитание
z2 = a - b              # вычитание
z1 = tf.divide(a, b)    # деление (поэлементное)
z2 = a / b              # деление (поэлементное)
z1 = tf.multiply(a, b)  # умножение (поэлементное)
z2 = a * b              # умножение (поэлементное)
z1 = a ** 2             # возведение в степень (поэлементное)
 
z1 = tf.tensordot(a, b, axes=0) # векторное внешнее умножение
z2 = tf.tensordot(a, b, axes=1) # векторное внутреннее умножение
 
a2 = tf.constant(tf.range(1, 10), shape=(3, 3))
b2 = tf.constant(tf.range(5, 14), shape=(3, 3))
 
z1 = tf.matmul(a2, b2)  # матричное умножение
z2 = a2 @ b2            # матричное умножение
print(z1, z2, sep="\n\n")
```

## Основные математические функции

Наиболее часто можно встретить функции для вычисления суммы, минимального, максимального и среднего значений. Давайте предположим, что у нас есть матрица:

```python
m = tf.tensordot(a, b, axes=0)  # матрица 3x3
```

Тогда сумму значений ее элементов можно подсчитать просто вызвав функцию:

```python
z = tf.reduce_sum(m)        # вычисление суммы элементов (по всем осям)
```

Если же нам нужно вычислить сумму только по столбцам, то для этого следует указать ось axis=0:

```python
z = tf.reduce_sum(m, axis=0)    # вычисление суммы элементов (при axis=0 суммируются строки)
```

Может удивить, что axis=0 – это суммирование по столбцам, а не по строкам. В действительности, этот параметр указывает по какой оси перебирать данные в тензоре и поэлементно суммировать между собой. В данном случае выбираются строки и суммируются, получается эффект суммирования по столбцам.

При необходимости можно указать несколько осей, например, так:

```python
z = tf.reduce_sum(m, axis=[0, 1])    # вычисление суммы элементов (по axis=0 и 1)
```

это будет эквивалентно суммированию всех элементов в матрице m.

Аналогично работают следующие математические функции:

```python
z = tf.reduce_mean(m)	     # среднее арифметическое
z = tf.reduce_max(m, axis=0)      # максимальное по столбцам
z = tf.reduce_min(m, axis=1)      # минимальное по строкам
z = tf.reduce_prod(m)             # произведение значений элементов матрицы
```

Есть и другие математические функции, например, вычисление квадратного корня и возведение в квадрат:

```python
z = tf.sqrt(tf.cast(a, dtype=tf.float32))   # работает только с вещественными типами
z = tf.square(a)        # возведение в квадрат
```

Также существуют все тригонометрические функции:

```python
z = tf.sin(tf.range(-3.14, 3.14, 1))    # синус от значений тензора
z = tf.cos(tf.range(-3.14, 3.14, 1))    # косинус от значений тензора
```

Кроме того, имеются все распространенные специальные функции, используемые в нейронных сетях:

```python
z = tf.keras.activations.relu(a)
z = tf.keras.activations.sigmoid(tf.cast(a, dtype=tf.float32))
z = tf.keras.activations.linear(a)
```